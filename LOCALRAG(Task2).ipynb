# Import necessary libraries
import requests
from bs4 import BeautifulSoup
import openai
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Setup OpenAI API Key
openai.api_key = 'your_openai_api_key_here'

# Step 1: Web Scraping and Content Extraction
def scrape_website(url):
    """Crawl and scrape content from a given URL."""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract text content (you can customize this to target specific tags)
    paragraphs = soup.find_all('p')
    text_content = ' '.join([p.get_text() for p in paragraphs])
    
    return text_content

# Step 2: Data Ingestion - Create Embeddings and Store in Vector Database
def create_embeddings(text_data):
    """Convert text data into embeddings using a pre-trained model."""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(text_data)
    return embeddings

# Step 3: Store Embeddings in Vector Database (FAISS)
class VectorDatabase:
    def __init__(self, dimension=768):
        """Initialize the vector database using FAISS."""
        self.dimension = dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.metadata = []

    def add(self, embeddings, metadata):
        """Add embeddings and metadata to the database."""
        self.index.add(np.array(embeddings))
        self.metadata.extend(metadata)

    def search(self, query_embedding, top_k=3):
        """Search the vector database for the most similar embeddings."""
        distances, indices = self.index.search(np.array([query_embedding]), top_k)
        return distances, indices

# Initialize the vector database
vector_db = VectorDatabase()

# Step 4: Process the Website Content and Add to Vector Database
def process_and_store_website(url):
    """Process website and store embeddings."""
    content = scrape_website(url)
    chunks = content.split('.')  # Simple segmentation based on sentence endings
    embeddings = create_embeddings(chunks)
    vector_db.add(embeddings, chunks)

# Example usage for crawling and processing a website
# process_and_store_website('https://example.com')

# Step 5: Handle User Query by Creating Embedding and Searching Database
def query_handler(user_query):
    """Process the user query and retrieve relevant information."""
    query_embedding = create_embeddings([user_query])[0]
    distances, indices = vector_db.search(query_embedding)

    # Retrieve the most relevant chunks based on the search results
    relevant_chunks = [vector_db.metadata[idx] for idx in indices[0]]

    return relevant_chunks

# Step 6: Response Generation Using LLM (OpenAI API)
def generate_response(user_query, relevant_chunks):
    """Generate a response using OpenAI's GPT model."""
    # Combine the retrieved chunks with the user query to generate a detailed response
    context = " ".join(relevant_chunks)
    prompt = f"User's question: {user_query}\nContext: {context}\nAnswer:"

    response = openai.Completion.create(
        model="text-davinci-003",  # You can change to a different model
        prompt=prompt,
        max_tokens=150,
        n=1,
        stop=None,
        temperature=0.7,
    )

    return response.choices[0].text.strip()

# Step 7: Example User Query Flow
def answer_user_query(url, user_query):
    """Full flow to scrape website, store embeddings, and generate response to a query."""
    # Process and store website content in the vector database
    process_and_store_website(url)

    # Handle the user query and retrieve relevant chunks
    relevant_chunks = query_handler(user_query)

    # Generate a response using the relevant chunks
    response = generate_response(user_query, relevant_chunks)
    return response

# Example query handling
url = 'https://example.com'
user_query = 'What is the main idea of the website?'

# Call the answer_user_query function
response = answer_user_query(url, user_query)
print(response)
