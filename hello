
Install Necessary Libraries:
pip install beautifulsoup4 requests sentence-transformers faiss-cpu openai
Scrape Data from a Website:
import requests
from bs4 import BeautifulSoup

def scrape_website(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract all paragraphs from the website
    paragraphs = [p.get_text() for p in soup.find_all('p')]
    
    # Optional: Extract headings (e.g., h1, h2, h3)
    headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])]
    
    # Combine paragraphs and headings to form chunks
    content = paragraphs + headings
    return content

url = "https://example.com"  # Replace with the website you want to scrape
website_data = scrape_website(url)

Embed the Scraped Data:
from sentence_transformers import SentenceTransformer
import numpy as np

# Initialize the embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_data(data):
    # Embed the chunks (text data from the website)
    embeddings = model.encode(data)
    return np.array(embeddings)

embeddings = embed_data(website_data)

 Store Embeddings in FAISS:
import faiss

def create_faiss_index(embeddings):
    # Create a FAISS index for similarity search
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance for similarity search
    index.add(embeddings)  # Add embeddings to the index
    return index

index = create_faiss_index(embeddings)

Query Handling (Search and Retrieve Similar Chunks):
def search_query(query, model, index, data):
    # Convert the query into an embedding
    query_embedding = model.encode([query])
    
    # Perform the search for the top N most similar chunks
    D, I = index.search(query_embedding, k=5)  # Search for 5 closest matches
    
    # Retrieve the relevant chunks based on the indices
    relevant_chunks = [data[i] for i in I[0]]
    return relevant_chunks

# Example query
user_query = "What is the purpose of web scraping?"
retrieved_chunks = search_query(user_query, model, index, website_data)

# Print the retrieved chunks
for chunk in retrieved_chunks:
    print(chunk)


import openai

openai.api_key = "your-openai-api-key"  # Add your OpenAI API key

def generate_response(query, relevant_chunks):
    context = "\n".join(relevant_chunks)  # Combine relevant chunks for context
    

    prompt = f"Answer the following question using the provided context:\n\nContext: {context}\n\nQuestion: {query}\nAnswer:"
    
    response = openai.Completion.create(
        engine="text-davinci-003",  # Use the GPT-3 engine
        prompt=prompt,
        max_tokens=150
    )
    
    return response.choices[0].text.strip()

# Generate a response for the user query
response = generate_response(user_query, retrieved_chunks)
print(response)
